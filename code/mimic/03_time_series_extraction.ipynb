{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Data Extraction for ARDS Cohort\n",
    "\n",
    "This notebook extracts all time series variables from the analysis_dataset_schema.png for our ARDS cohort:\n",
    "- Demographics and static variables\n",
    "- Ventilation parameters (PEEP, FiO2, SpO2, etc.)\n",
    "- Neuromuscular blockade agents (doses)\n",
    "- Proning events\n",
    "- Outcomes (mortality, LOS, extubation)\n",
    "- All timestamped relative to ARDS onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define paths\n",
    "MIMIC_PATH = '/Users/kavenchhikara/Desktop/CLIF/MIMIC-IV-3.1/physionet.org/files'\n",
    "DATA_PATH = '/Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/ards_analysis/data'\n",
    "MAPPING_PATH = '/Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/mimic_mapping.xlsx'\n",
    "\n",
    "print(f\"Analysis start time: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load ARDS Cohort and Mapping File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ARDS cohort\n",
    "ards_cohort = pd.read_parquet(f'{DATA_PATH}/ards_cohort.parquet')\n",
    "print(f\"ARDS cohort loaded: {len(ards_cohort):,} patients\")\n",
    "\n",
    "# Convert datetime columns\n",
    "datetime_cols = ['admission_dttm', 'discharge_dttm', 'ards_onset_time', 'bilateral_infiltrates_time']\n",
    "for col in datetime_cols:\n",
    "    ards_cohort[col] = pd.to_datetime(ards_cohort[col])\n",
    "\n",
    "# Load MIMIC mapping\n",
    "mimic_mapping = pd.read_excel(MAPPING_PATH)\n",
    "print(f\"\\nMIMIC mapping loaded: {len(mimic_mapping)} variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display mapping for key variables\n",
    "print(\"Neuromuscular blockade agents:\")\n",
    "nmb_agents = ['cisatracurium', 'vecuronium', 'rocuronium', 'pancuronium', 'atracurium']\n",
    "nmb_mapping = mimic_mapping[mimic_mapping['variable'].isin(nmb_agents)]\n",
    "print(nmb_mapping[['variable', 'itemid', 'label', 'unitname']])\n",
    "\n",
    "print(\"\\nVentilation parameters:\")\n",
    "vent_vars = ['fio2_set', 'peep_set', 'spo2', 'mode_name', 'device_name']\n",
    "vent_mapping = mimic_mapping[mimic_mapping['variable'].isin(vent_vars)]\n",
    "print(vent_mapping[['variable', 'itemid', 'label']])\n",
    "\n",
    "print(\"\\nPosition/Proning:\")\n",
    "position_mapping = mimic_mapping[mimic_mapping['variable'] == 'position']\n",
    "print(position_mapping[['variable', 'itemid', 'label', 'value_instances']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Demographics and Static Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ICU stays for APACHE scores\n",
    "icustays = pd.read_csv(f'{MIMIC_PATH}/mimiciv/3.1/icu/icustays.csv.gz')\n",
    "first_day_apache = pd.read_csv(f'{MIMIC_PATH}/mimiciv/3.1/icu/first_day_apache_ii.csv.gz')\n",
    "\n",
    "# Merge APACHE scores\n",
    "ards_with_apache = ards_cohort.merge(\n",
    "    icustays[['hadm_id', 'stay_id']], \n",
    "    on='hadm_id', \n",
    "    how='left'\n",
    ").merge(\n",
    "    first_day_apache[['stay_id', 'apache_ii']], \n",
    "    on='stay_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Get first ICU stay APACHE score for each admission\n",
    "apache_scores = ards_with_apache.groupby('hadm_id')['apache_ii'].first().reset_index()\n",
    "apache_scores.rename(columns={'apache_ii': 'APACHE'}, inplace=True)\n",
    "\n",
    "print(f\"APACHE scores available for {apache_scores['APACHE'].notna().sum()} patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract height and weight\n",
    "print(\"Extracting height and weight...\")\n",
    "\n",
    "# Get height/weight itemids from mapping\n",
    "height_itemids = mimic_mapping[mimic_mapping['variable'] == 'height_cm']['itemid'].dropna().astype(int).tolist()\n",
    "weight_itemids = mimic_mapping[mimic_mapping['variable'].isin(['weight_kg', 'weight_lbs'])]['itemid'].dropna().astype(int).tolist()\n",
    "\n",
    "# Extract from chartevents\n",
    "height_weight_data = []\n",
    "\n",
    "# Get ICU stays for our cohort\n",
    "cohort_stays = icustays[icustays['hadm_id'].isin(ards_cohort['hadm_id'])]['stay_id'].tolist()\n",
    "\n",
    "chunk_size = 1000000\n",
    "for i, chunk in enumerate(pd.read_csv(f'{MIMIC_PATH}/mimiciv/3.1/icu/chartevents.csv.gz', \n",
    "                                     chunksize=chunk_size,\n",
    "                                     usecols=['stay_id', 'itemid', 'valuenum', 'valueuom'])):\n",
    "    # Filter for our cohort and height/weight items\n",
    "    hw_chunk = chunk[\n",
    "        (chunk['stay_id'].isin(cohort_stays)) &\n",
    "        (chunk['itemid'].isin(height_itemids + weight_itemids))\n",
    "    ]\n",
    "    \n",
    "    if len(hw_chunk) > 0:\n",
    "        height_weight_data.append(hw_chunk)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processed {(i+1)*chunk_size:,} rows...\")\n",
    "\n",
    "# Combine and process\n",
    "if height_weight_data:\n",
    "    hw_df = pd.concat(height_weight_data, ignore_index=True)\n",
    "    hw_df = hw_df.merge(icustays[['stay_id', 'hadm_id']], on='stay_id', how='left')\n",
    "    \n",
    "    # Process height (convert to cm if needed)\n",
    "    height_df = hw_df[hw_df['itemid'].isin(height_itemids)].groupby('hadm_id')['valuenum'].first().reset_index()\n",
    "    height_df.rename(columns={'valuenum': 'height_cm'}, inplace=True)\n",
    "    \n",
    "    # Process weight (convert to kg if needed)\n",
    "    weight_df = hw_df[hw_df['itemid'].isin(weight_itemids)].copy()\n",
    "    # Convert lbs to kg where applicable\n",
    "    lbs_mask = weight_df['valueuom'] == 'lbs'\n",
    "    weight_df.loc[lbs_mask, 'valuenum'] = weight_df.loc[lbs_mask, 'valuenum'] * 0.453592\n",
    "    weight_df = weight_df.groupby('hadm_id')['valuenum'].first().reset_index()\n",
    "    weight_df.rename(columns={'valuenum': 'weight_kg'}, inplace=True)\n",
    "    \n",
    "    print(f\"\\nHeight data found for {len(height_df)} patients\")\n",
    "    print(f\"Weight data found for {len(weight_df)} patients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract Time Series Ventilation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously extracted ventilation data\n",
    "vent_data = pd.read_parquet(f'{DATA_PATH}/ventilation_parameters.parquet')\n",
    "vent_data['charttime'] = pd.to_datetime(vent_data['charttime'])\n",
    "\n",
    "# Get additional ventilation parameters from mapping\n",
    "additional_vent_itemids = {\n",
    "    'fio2_set': mimic_mapping[mimic_mapping['variable'] == 'fio2_set']['itemid'].dropna().astype(int).tolist(),\n",
    "    'peep_set': mimic_mapping[mimic_mapping['variable'] == 'peep_set']['itemid'].dropna().astype(int).tolist(),\n",
    "    'spo2': mimic_mapping[mimic_mapping['variable'] == 'spo2']['itemid'].dropna().astype(int).tolist(),\n",
    "    'device_name': mimic_mapping[mimic_mapping['variable'] == 'device_name']['itemid'].dropna().astype(int).tolist(),\n",
    "    'ecmo_flag': mimic_mapping[mimic_mapping['variable'] == 'device_name_ecmo']['itemid'].dropna().astype(int).tolist()\n",
    "}\n",
    "\n",
    "print(\"Extracting additional ventilation parameters...\")\n",
    "additional_vent_data = []\n",
    "\n",
    "all_additional_itemids = [item for items in additional_vent_itemids.values() for item in items]\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(f'{MIMIC_PATH}/mimiciv/3.1/icu/chartevents.csv.gz', \n",
    "                                     chunksize=chunk_size)):\n",
    "    # Filter for our cohort and items\n",
    "    chunk_filtered = chunk[\n",
    "        (chunk['stay_id'].isin(cohort_stays)) &\n",
    "        (chunk['itemid'].isin(all_additional_itemids))\n",
    "    ]\n",
    "    \n",
    "    if len(chunk_filtered) > 0:\n",
    "        additional_vent_data.append(chunk_filtered)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processed {(i+1)*chunk_size:,} rows...\")\n",
    "\n",
    "# Process additional ventilation data\n",
    "if additional_vent_data:\n",
    "    add_vent_df = pd.concat(additional_vent_data, ignore_index=True)\n",
    "    add_vent_df = add_vent_df.merge(icustays[['stay_id', 'hadm_id']], on='stay_id', how='left')\n",
    "    add_vent_df['charttime'] = pd.to_datetime(add_vent_df['charttime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Extract Neuromuscular Blockade Administration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get NMB itemids from mapping\n",
    "nmb_itemids = {}\n",
    "for agent in nmb_agents:\n",
    "    itemids = mimic_mapping[mimic_mapping['variable'] == agent]['itemid'].dropna().astype(int).tolist()\n",
    "    if itemids:\n",
    "        nmb_itemids[agent] = itemids\n",
    "\n",
    "print(\"Neuromuscular blockade itemids:\")\n",
    "for agent, ids in nmb_itemids.items():\n",
    "    print(f\"{agent}: {ids}\")\n",
    "\n",
    "# Extract NMB administration from inputevents_mv\n",
    "print(\"\\nExtracting NMB administration data...\")\n",
    "all_nmb_itemids = [item for items in nmb_itemids.values() for item in items]\n",
    "\n",
    "nmb_data = []\n",
    "for i, chunk in enumerate(pd.read_csv(f'{MIMIC_PATH}/mimiciv/3.1/icu/inputevents.csv.gz', \n",
    "                                     chunksize=chunk_size)):\n",
    "    # Filter for NMB medications\n",
    "    nmb_chunk = chunk[\n",
    "        (chunk['stay_id'].isin(cohort_stays)) &\n",
    "        (chunk['itemid'].isin(all_nmb_itemids))\n",
    "    ]\n",
    "    \n",
    "    if len(nmb_chunk) > 0:\n",
    "        nmb_data.append(nmb_chunk)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processed {(i+1)*chunk_size:,} rows...\")\n",
    "\n",
    "# Process NMB data\n",
    "if nmb_data:\n",
    "    nmb_df = pd.concat(nmb_data, ignore_index=True)\n",
    "    nmb_df = nmb_df.merge(icustays[['stay_id', 'hadm_id']], on='stay_id', how='left')\n",
    "    nmb_df['starttime'] = pd.to_datetime(nmb_df['starttime'])\n",
    "    nmb_df['endtime'] = pd.to_datetime(nmb_df['endtime'])\n",
    "    \n",
    "    # Map itemids back to drug names\n",
    "    itemid_to_drug = {}\n",
    "    for drug, itemids in nmb_itemids.items():\n",
    "        for itemid in itemids:\n",
    "            itemid_to_drug[itemid] = drug\n",
    "    \n",
    "    nmb_df['drug_name'] = nmb_df['itemid'].map(itemid_to_drug)\n",
    "    \n",
    "    print(f\"\\nNMB administrations found: {len(nmb_df):,}\")\n",
    "    print(\"\\nNMB drugs administered:\")\n",
    "    print(nmb_df['drug_name'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Extract Proning Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get position itemids from mapping\n",
    "position_itemids = mimic_mapping[mimic_mapping['variable'] == 'position']['itemid'].dropna().astype(int).tolist()\n",
    "print(f\"Position itemids: {position_itemids}\")\n",
    "\n",
    "# Extract position data\n",
    "print(\"\\nExtracting position/proning data...\")\n",
    "position_data = []\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(f'{MIMIC_PATH}/mimiciv/3.1/icu/chartevents.csv.gz', \n",
    "                                     chunksize=chunk_size,\n",
    "                                     usecols=['stay_id', 'itemid', 'charttime', 'value'])):\n",
    "    # Filter for position items\n",
    "    pos_chunk = chunk[\n",
    "        (chunk['stay_id'].isin(cohort_stays)) &\n",
    "        (chunk['itemid'].isin(position_itemids))\n",
    "    ]\n",
    "    \n",
    "    if len(pos_chunk) > 0:\n",
    "        position_data.append(pos_chunk)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processed {(i+1)*chunk_size:,} rows...\")\n",
    "\n",
    "# Process position data\n",
    "if position_data:\n",
    "    position_df = pd.concat(position_data, ignore_index=True)\n",
    "    position_df = position_df.merge(icustays[['stay_id', 'hadm_id']], on='stay_id', how='left')\n",
    "    position_df['charttime'] = pd.to_datetime(position_df['charttime'])\n",
    "    \n",
    "    # Identify proning events\n",
    "    prone_keywords = ['prone', 'proning', 'pronation']\n",
    "    position_df['is_prone'] = position_df['value'].str.lower().str.contains('|'.join(prone_keywords), na=False)\n",
    "    \n",
    "    prone_events = position_df[position_df['is_prone']]\n",
    "    print(f\"\\nProne position events found: {len(prone_events):,}\")\n",
    "    print(f\"Patients with proning: {prone_events['hadm_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Extract Tracheostomy Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load procedures to identify tracheostomy\n",
    "procedures = pd.read_csv(f'{MIMIC_PATH}/mimiciv/3.1/icu/procedureevents.csv.gz')\n",
    "\n",
    "# Get tracheostomy itemids from mapping\n",
    "trach_itemids = mimic_mapping[mimic_mapping['variable'] == 'tracheostomy']['itemid'].dropna().astype(int).tolist()\n",
    "print(f\"Tracheostomy itemids: {trach_itemids}\")\n",
    "\n",
    "# Filter for tracheostomy procedures\n",
    "trach_procedures = procedures[\n",
    "    (procedures['stay_id'].isin(cohort_stays)) &\n",
    "    (procedures['itemid'].isin(trach_itemids))\n",
    "]\n",
    "\n",
    "if len(trach_procedures) > 0:\n",
    "    trach_procedures = trach_procedures.merge(icustays[['stay_id', 'hadm_id']], on='stay_id', how='left')\n",
    "    trach_procedures['starttime'] = pd.to_datetime(trach_procedures['starttime'])\n",
    "    \n",
    "    # Get first tracheostomy for each admission\n",
    "    first_trach = trach_procedures.groupby('hadm_id')['starttime'].min().reset_index()\n",
    "    first_trach.rename(columns={'starttime': 'tracheostomy_time'}, inplace=True)\n",
    "    \n",
    "    print(f\"\\nPatients with tracheostomy: {len(first_trach)}\")\n",
    "else:\n",
    "    first_trach = pd.DataFrame(columns=['hadm_id', 'tracheostomy_time'])\n",
    "    print(\"\\nNo tracheostomy procedures found in cohort\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create Time Series Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create hourly time series\n",
    "def create_time_series_data(hadm_id, ards_onset_time, admission_time, discharge_time):\n",
    "    \"\"\"\n",
    "    Create hourly time series for a patient relative to ARDS onset\n",
    "    \"\"\"\n",
    "    # Create hourly timestamps from admission to discharge\n",
    "    hours = pd.date_range(start=admission_time, end=discharge_time, freq='H')\n",
    "    \n",
    "    # Create base dataframe\n",
    "    ts_data = pd.DataFrame({\n",
    "        'hadm_id': hadm_id,\n",
    "        'recorded_dttm': hours,\n",
    "        'time_from_ARDS_onset': (hours - ards_onset_time).total_seconds() / 3600  # hours from ARDS onset\n",
    "    })\n",
    "    \n",
    "    return ts_data\n",
    "\n",
    "# Create time series framework for all ARDS patients\n",
    "print(\"Creating time series framework...\")\n",
    "all_ts_data = []\n",
    "\n",
    "for idx, row in ards_cohort.iterrows():\n",
    "    ts_data = create_time_series_data(\n",
    "        row['hadm_id'],\n",
    "        row['ards_onset_time'],\n",
    "        row['admission_dttm'],\n",
    "        row['discharge_dttm']\n",
    "    )\n",
    "    all_ts_data.append(ts_data)\n",
    "    \n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processed {idx} patients...\")\n",
    "\n",
    "# Combine all time series\n",
    "time_series_df = pd.concat(all_ts_data, ignore_index=True)\n",
    "print(f\"\\nTotal time series rows created: {len(time_series_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add static variables\n",
    "print(\"Adding static variables...\")\n",
    "\n",
    "# Merge demographics from ARDS cohort\n",
    "static_vars = ards_cohort[[\n",
    "    'hadm_id', 'subject_id', 'hospital_id', 'hospitalization_id',\n",
    "    'admission_dttm', 'discharge_dttm', 'age_at_admission', \n",
    "    'gender', 'ethnicity', 'ards_severity'\n",
    "]].copy()\n",
    "\n",
    "# Rename columns to match schema\n",
    "static_vars.rename(columns={\n",
    "    'gender': 'sex',\n",
    "    'hadm_id': 'hospitalization_id_temp'\n",
    "}, inplace=True)\n",
    "static_vars['hospital_id'] = 'BIDMC'  # All MIMIC data is from BIDMC\n",
    "static_vars['patient_id'] = static_vars['subject_id']\n",
    "static_vars['hospitalization_id'] = static_vars['hospitalization_id_temp']\n",
    "\n",
    "# Add APACHE scores\n",
    "static_vars = static_vars.merge(apache_scores, left_on='hospitalization_id_temp', right_on='hadm_id', how='left')\n",
    "\n",
    "# Add height and weight\n",
    "if 'height_df' in locals():\n",
    "    static_vars = static_vars.merge(height_df, left_on='hospitalization_id_temp', right_on='hadm_id', how='left')\n",
    "if 'weight_df' in locals():\n",
    "    static_vars = static_vars.merge(weight_df, left_on='hospitalization_id_temp', right_on='hadm_id', how='left')\n",
    "\n",
    "# Merge with time series\n",
    "time_series_df = time_series_df.merge(\n",
    "    static_vars.drop(columns=['hospitalization_id_temp', 'hadm_id_x', 'hadm_id_y'], errors='ignore'),\n",
    "    left_on='hadm_id',\n",
    "    right_on='hospitalization_id',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time-varying ventilation parameters\n",
    "print(\"\\nAdding time-varying ventilation parameters...\")\n",
    "\n",
    "# Function to get nearest value within time window\n",
    "def get_nearest_value(ts_row, value_df, value_col, time_col='charttime', window_hours=1):\n",
    "    \"\"\"\n",
    "    Get nearest value within specified time window\n",
    "    \"\"\"\n",
    "    hadm_id = ts_row['hadm_id']\n",
    "    target_time = ts_row['recorded_dttm']\n",
    "    \n",
    "    # Filter to same admission\n",
    "    hadm_values = value_df[value_df['hadm_id'] == hadm_id]\n",
    "    \n",
    "    if len(hadm_values) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    # Calculate time differences\n",
    "    time_diff = abs((hadm_values[time_col] - target_time).dt.total_seconds() / 3600)\n",
    "    \n",
    "    # Find values within window\n",
    "    within_window = time_diff <= window_hours\n",
    "    \n",
    "    if within_window.sum() == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    # Return value with minimum time difference\n",
    "    nearest_idx = time_diff[within_window].idxmin()\n",
    "    return hadm_values.loc[nearest_idx, value_col]\n",
    "\n",
    "# Add ventilation parameters (sample for first 1000 rows due to computational intensity)\n",
    "sample_size = min(1000, len(time_series_df))\n",
    "print(f\"Processing sample of {sample_size} rows for demonstration...\")\n",
    "\n",
    "# For full dataset, you would remove the .head(sample_size)\n",
    "for param in ['peep', 'fio2', 'pao2', 'pf_ratio']:\n",
    "    if param in vent_data.columns:\n",
    "        print(f\"Adding {param}...\")\n",
    "        time_series_df.loc[:sample_size-1, param] = time_series_df.head(sample_size).apply(\n",
    "            lambda x: get_nearest_value(x, vent_data, param), axis=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add NMB doses\n",
    "print(\"\\nCalculating NMB doses...\")\n",
    "\n",
    "if 'nmb_df' in locals() and len(nmb_df) > 0:\n",
    "    # Calculate hourly doses for each NMB agent\n",
    "    for drug in nmb_agents:\n",
    "        drug_data = nmb_df[nmb_df['drug_name'] == drug]\n",
    "        \n",
    "        if len(drug_data) > 0:\n",
    "            # Initialize dose column\n",
    "            time_series_df[f'{drug}_dose'] = 0.0\n",
    "            \n",
    "            # For each administration, calculate hourly dose\n",
    "            for _, admin in drug_data.iterrows():\n",
    "                # Find rows within administration period\n",
    "                mask = (\n",
    "                    (time_series_df['hadm_id'] == admin['hadm_id']) &\n",
    "                    (time_series_df['recorded_dttm'] >= admin['starttime']) &\n",
    "                    (time_series_df['recorded_dttm'] <= admin['endtime'])\n",
    "                )\n",
    "                \n",
    "                if mask.sum() > 0:\n",
    "                    # Calculate dose rate (amount/hour)\n",
    "                    duration_hours = (admin['endtime'] - admin['starttime']).total_seconds() / 3600\n",
    "                    if duration_hours > 0 and pd.notna(admin['amount']):\n",
    "                        hourly_dose = admin['amount'] / duration_hours\n",
    "                        \n",
    "                        # Apply dose to matching rows\n",
    "                        time_series_df.loc[mask, f'{drug}_dose'] += hourly_dose\n",
    "else:\n",
    "    # Initialize NMB dose columns as 0\n",
    "    for drug in nmb_agents:\n",
    "        time_series_df[f'{drug}_dose'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add proning flag\n",
    "print(\"\\nAdding proning flags...\")\n",
    "\n",
    "time_series_df['prone_flag'] = 0\n",
    "\n",
    "if 'prone_events' in locals() and len(prone_events) > 0:\n",
    "    # For each prone event, mark the time period\n",
    "    for hadm_id in prone_events['hadm_id'].unique():\n",
    "        hadm_prone = prone_events[prone_events['hadm_id'] == hadm_id].sort_values('charttime')\n",
    "        \n",
    "        # Mark hours when patient was prone\n",
    "        for _, event in hadm_prone.iterrows():\n",
    "            # Mark 4 hours around each prone documentation (assuming prone sessions last ~4 hours)\n",
    "            mask = (\n",
    "                (time_series_df['hadm_id'] == hadm_id) &\n",
    "                (time_series_df['recorded_dttm'] >= event['charttime'] - pd.Timedelta(hours=2)) &\n",
    "                (time_series_df['recorded_dttm'] <= event['charttime'] + pd.Timedelta(hours=2))\n",
    "            )\n",
    "            time_series_df.loc[mask, 'prone_flag'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add outcome variables\n",
    "print(\"\\nAdding outcome variables...\")\n",
    "\n",
    "# Hospital mortality (from ARDS cohort)\n",
    "mortality_map = ards_cohort.set_index('hadm_id')['hospital_expire_flag'].to_dict()\n",
    "time_series_df['hospital_mortality'] = time_series_df['hadm_id'].map(mortality_map)\n",
    "\n",
    "# Disposition category\n",
    "disposition_map = ards_cohort.set_index('hadm_id')['discharge_location'].to_dict()\n",
    "time_series_df['disposition_category'] = time_series_df['hadm_id'].map(disposition_map)\n",
    "\n",
    "# Categorize disposition\n",
    "def categorize_disposition(location):\n",
    "    if pd.isna(location):\n",
    "        return 'Unknown'\n",
    "    location_lower = location.lower()\n",
    "    if 'expired' in location_lower or 'died' in location_lower:\n",
    "        return 'Expired'\n",
    "    elif 'hospice' in location_lower:\n",
    "        return 'Hospice'\n",
    "    elif 'home' in location_lower:\n",
    "        return 'Home'\n",
    "    elif 'snf' in location_lower or 'skilled' in location_lower:\n",
    "        return 'Facility'\n",
    "    elif 'rehab' in location_lower:\n",
    "        return 'Facility'\n",
    "    else:\n",
    "        return 'Transfer to another facility'\n",
    "\n",
    "time_series_df['disposition_category'] = time_series_df['disposition_category'].apply(categorize_disposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new_tracheostomy flag\n",
    "print(\"\\nAdding tracheostomy flags...\")\n",
    "\n",
    "time_series_df['new_tracheostomy'] = 0\n",
    "\n",
    "if len(first_trach) > 0:\n",
    "    # Mark new tracheostomy after it occurs\n",
    "    for _, trach in first_trach.iterrows():\n",
    "        mask = (\n",
    "            (time_series_df['hadm_id'] == trach['hadm_id']) &\n",
    "            (time_series_df['recorded_dttm'] >= trach['tracheostomy_time'])\n",
    "        )\n",
    "        time_series_df.loc[mask, 'new_tracheostomy'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Final Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and order columns according to schema\n",
    "schema_columns = [\n",
    "    'hospital_id', 'patient_id', 'hospitalization_id', 'recorded_dttm',\n",
    "    'time_from_ARDS_onset', 'APACHE', 'sex', 'age_at_admission', 'ethnicity',\n",
    "    'disposition_category', 'respiratory_device', 'ecmo_flag',\n",
    "    'pao2', 'fio2_set', 'lmp_set', 'spo2', 'peep',\n",
    "    'height_cm', 'weight_kg',\n",
    "    'cisatracurium_dose', 'vecuronium_dose', 'rocuronium_dose',\n",
    "    'atracurium_dose', 'pancuronium_dose',\n",
    "    'prone_flag', 'new_tracheostomy'\n",
    "]\n",
    "\n",
    "# Map column names\n",
    "column_mapping = {\n",
    "    'fio2': 'fio2_set',\n",
    "    'hadm_id': 'hospitalization_id'\n",
    "}\n",
    "\n",
    "# Apply mapping\n",
    "time_series_df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# Add missing columns with default values\n",
    "for col in schema_columns:\n",
    "    if col not in time_series_df.columns:\n",
    "        if col == 'respiratory_device':\n",
    "            time_series_df[col] = 'Vent'  # Assume ventilated for ARDS patients\n",
    "        elif col == 'ecmo_flag':\n",
    "            time_series_df[col] = 0\n",
    "        elif col == 'lmp_set':\n",
    "            time_series_df[col] = np.nan\n",
    "        else:\n",
    "            time_series_df[col] = np.nan\n",
    "\n",
    "# Select columns in schema order (only those that exist)\n",
    "final_columns = [col for col in schema_columns if col in time_series_df.columns]\n",
    "final_time_series = time_series_df[final_columns].copy()\n",
    "\n",
    "print(\"Final dataset shape:\", final_time_series.shape)\n",
    "print(\"\\nColumns in final dataset:\")\n",
    "print(final_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final time series dataset\n",
    "output_file = f'{DATA_PATH}/ards_time_series_analysis.parquet'\n",
    "final_time_series.to_parquet(output_file, index=False)\n",
    "print(f\"\\nTime series dataset saved to: {output_file}\")\n",
    "print(f\"File size: {os.path.getsize(output_file) / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n=== DATASET SUMMARY ===\")\n",
    "print(f\"Total rows: {len(final_time_series):,}\")\n",
    "print(f\"Unique patients: {final_time_series['patient_id'].nunique():,}\")\n",
    "print(f\"Unique hospitalizations: {final_time_series['hospitalization_id'].nunique():,}\")\n",
    "print(f\"\\nTime range relative to ARDS onset:\")\n",
    "print(f\"Min: {final_time_series['time_from_ARDS_onset'].min():.1f} hours\")\n",
    "print(f\"Max: {final_time_series['time_from_ARDS_onset'].max():.1f} hours\")\n",
    "\n",
    "print(\"\\nProne positioning:\")\n",
    "prone_patients = final_time_series[final_time_series['prone_flag'] == 1]['hospitalization_id'].nunique()\n",
    "print(f\"Patients with proning: {prone_patients} ({prone_patients/final_time_series['hospitalization_id'].nunique()*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nNeuromuscular blockade:\")\n",
    "for drug in nmb_agents:\n",
    "    drug_col = f'{drug}_dose'\n",
    "    if drug_col in final_time_series.columns:\n",
    "        drug_patients = final_time_series[final_time_series[drug_col] > 0]['hospitalization_id'].nunique()\n",
    "        print(f\"{drug}: {drug_patients} patients\")\n",
    "\n",
    "print(f\"\\nAnalysis completed at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The time series dataset is now ready for analysis:\n",
    "- Each row represents one hour of patient data\n",
    "- All times are relative to ARDS onset (time=0)\n",
    "- Includes all variables from the schema\n",
    "- Ready for statistical modeling and outcome analysis\n",
    "\n",
    "Note: For production use, remove the sampling limitation in Step 7 to process all rows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}