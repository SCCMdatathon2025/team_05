{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARDS Cohort Definition - UPDATED CRITERIA\n",
    "\n",
    "This notebook identifies the cohort for analyzing timing of proning and neuromuscular blockade in ARDS patients.\n",
    "\n",
    "## NEW Inclusion Criteria:\n",
    "- Adults (≥18 years)\n",
    "- At least one ICU admission\n",
    "- PEEP ≥ 5 within first 48 hours of ICU admission\n",
    "- S/F ratio < 315 at least once (SpO2/FiO2)\n",
    "- At least one radiology report\n",
    "\n",
    "## Exclusion Criteria:\n",
    "- Pregnant patients\n",
    "- Patients with heart failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIMIC data path: /Users/kavenchhikara/Desktop/CLIF/MIMIC-IV-3.1/physionet.org/files\n",
      "Analysis start time: 2025-07-20 00:20:53.279952\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define MIMIC data path\n",
    "MIMIC_PATH = '/Users/kavenchhikara/Desktop/CLIF/MIMIC-IV-3.1/physionet.org/files'\n",
    "\n",
    "print(f\"MIMIC data path: {MIMIC_PATH}\")\n",
    "print(f\"Analysis start time: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Core Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading patient demographics...\n",
      "Total patients: 364,627\n",
      "\n",
      "Loading admissions...\n",
      "Total admissions: 546,028\n",
      "\n",
      "Loading ICU stays...\n",
      "Total ICU stays: 94,458\n"
     ]
    }
   ],
   "source": [
    "# Load patient demographics\n",
    "print(\"Loading patient demographics...\")\n",
    "patients = pd.read_csv(f'{MIMIC_PATH}/mimiciv/3.1/hosp/patients.csv.gz')\n",
    "print(f\"Total patients: {len(patients):,}\")\n",
    "\n",
    "# Load admissions\n",
    "print(\"\\nLoading admissions...\")\n",
    "admissions = pd.read_csv(f'{MIMIC_PATH}/mimiciv/3.1/hosp/admissions.csv.gz')\n",
    "print(f\"Total admissions: {len(admissions):,}\")\n",
    "\n",
    "# Load ICU stays\n",
    "print(\"\\nLoading ICU stays...\")\n",
    "icustays = pd.read_csv(f'{MIMIC_PATH}/mimiciv/3.1/icu/icustays.csv.gz')\n",
    "print(f\"Total ICU stays: {len(icustays):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Filter to Adult Patients with ICU Admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult admissions (≥18 years): 546,028\n",
      "Unique adult patients: 223,452\n",
      "\n",
      "Adult admissions with ICU stays: 85,242\n",
      "Unique patients with ICU stays: 65,366\n"
     ]
    }
   ],
   "source": [
    "# Calculate age at admission\n",
    "patients['anchor_year'] = pd.to_numeric(patients['anchor_year'])\n",
    "patients['anchor_age'] = pd.to_numeric(patients['anchor_age'])\n",
    "\n",
    "# Merge with admissions to get admission year\n",
    "admissions['admittime'] = pd.to_datetime(admissions['admittime'])\n",
    "admissions['admit_year'] = admissions['admittime'].dt.year\n",
    "\n",
    "# Merge patients with admissions\n",
    "patient_admissions = admissions.merge(patients[['subject_id', 'anchor_age', 'anchor_year', 'gender']], \n",
    "                                     on='subject_id', how='left')\n",
    "\n",
    "# Calculate age at admission\n",
    "patient_admissions['age_at_admission'] = (patient_admissions['anchor_age'] + \n",
    "                                         (patient_admissions['admit_year'] - patient_admissions['anchor_year']))\n",
    "\n",
    "# Filter adults only\n",
    "adult_admissions = patient_admissions[patient_admissions['age_at_admission'] >= 18].copy()\n",
    "print(f\"Adult admissions (≥18 years): {len(adult_admissions):,}\")\n",
    "print(f\"Unique adult patients: {adult_admissions['subject_id'].nunique():,}\")\n",
    "\n",
    "# NEW: Filter to admissions with at least one ICU stay\n",
    "icustays['intime'] = pd.to_datetime(icustays['intime'])\n",
    "icustays['outtime'] = pd.to_datetime(icustays['outtime'])\n",
    "\n",
    "# Get admissions with ICU stays\n",
    "admissions_with_icu = adult_admissions.merge(\n",
    "    icustays[['hadm_id', 'stay_id', 'intime', 'outtime']].drop_duplicates(['hadm_id']),\n",
    "    on='hadm_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"\\nAdult admissions with ICU stays: {len(admissions_with_icu):,}\")\n",
    "print(f\"Unique patients with ICU stays: {admissions_with_icu['subject_id'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chartevents for cohort filtering (vectorized approach)...\n",
      "ICU stays for adult cohort: 94,458\n",
      "Loading chartevents data (this may take a few minutes)...\n",
      "Total chartevents loaded: 432,997,491\n",
      "Relevant measurements for our cohort: 19,758,934\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     34\u001b[39m cohort_data[\u001b[33m'\u001b[39m\u001b[33mparam_type\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33munknown\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     35\u001b[39m cohort_data.loc[cohort_data[\u001b[33m'\u001b[39m\u001b[33mitemid\u001b[39m\u001b[33m'\u001b[39m].isin(peep_itemids), \u001b[33m'\u001b[39m\u001b[33mparam_type\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33mpeep\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m cohort_data.loc[\u001b[43mcohort_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mitemid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspo2_itemids\u001b[49m\u001b[43m)\u001b[49m, \u001b[33m'\u001b[39m\u001b[33mparam_type\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33mspo2\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     37\u001b[39m cohort_data.loc[cohort_data[\u001b[33m'\u001b[39m\u001b[33mitemid\u001b[39m\u001b[33m'\u001b[39m].isin(fio2_itemids), \u001b[33m'\u001b[39m\u001b[33mparam_type\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33mfio2\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Convert times and merge with ICU stay info\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages/pandas/core/series.py:5570\u001b[39m, in \u001b[36mSeries.isin\u001b[39m\u001b[34m(self, values)\u001b[39m\n\u001b[32m   5497\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34misin\u001b[39m(\u001b[38;5;28mself\u001b[39m, values) -> Series:\n\u001b[32m   5498\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5499\u001b[39m \u001b[33;03m    Whether elements in Series are contained in `values`.\u001b[39;00m\n\u001b[32m   5500\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5568\u001b[39m \u001b[33;03m    dtype: bool\u001b[39;00m\n\u001b[32m   5569\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5570\u001b[39m     result = \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor(result, index=\u001b[38;5;28mself\u001b[39m.index, copy=\u001b[38;5;28;01mFalse\u001b[39;00m).__finalize__(\n\u001b[32m   5572\u001b[39m         \u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33misin\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5573\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages/pandas/core/algorithms.py:545\u001b[39m, in \u001b[36misin\u001b[39m\u001b[34m(comps, values)\u001b[39m\n\u001b[32m    542\u001b[39m     comps_array = comps_array.astype(common, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    543\u001b[39m     f = htable.ismember\n\u001b[32m--> \u001b[39m\u001b[32m545\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomps_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages/pandas/core/algorithms.py:537\u001b[39m, in \u001b[36misin.<locals>.<lambda>\u001b[39m\u001b[34m(a, b)\u001b[39m\n\u001b[32m    534\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m np.logical_or(np.isin(c, v).ravel(), np.isnan(c))\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m         f = \u001b[38;5;28;01mlambda\u001b[39;00m a, b: \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m.ravel()\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    540\u001b[39m     common = np_find_common_type(values.dtype, comps_array.dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/SCCM/SCCM-Team2/.venv/lib/python3.13/site-packages/numpy/lib/_arraysetops_impl.py:1178\u001b[39m, in \u001b[36misin\u001b[39m\u001b[34m(element, test_elements, assume_unique, invert, kind)\u001b[39m\n\u001b[32m   1063\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1064\u001b[39m \u001b[33;03mCalculates ``element in test_elements``, broadcasting over `element` only.\u001b[39;00m\n\u001b[32m   1065\u001b[39m \u001b[33;03mReturns a boolean array of the same shape as `element` that is True\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1174\u001b[39m \u001b[33;03m       [ True, False]])\u001b[39;00m\n\u001b[32m   1175\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1176\u001b[39m element = np.asarray(element)\n\u001b[32m   1177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_in1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_elements\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massume_unique\u001b[49m\u001b[43m=\u001b[49m\u001b[43massume_unique\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1178\u001b[39m \u001b[43m             \u001b[49m\u001b[43minvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43minvert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkind\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Vectorized approach for extracting PEEP and S/F ratio criteria\n",
    "print(\"Loading chartevents for cohort filtering (vectorized approach)...\")\n",
    "\n",
    "# Define all itemids we need\n",
    "peep_itemids = [220339, 224700, 224699]  # PEEP set, Total PEEP, Auto PEEP\n",
    "spo2_itemids = [220277, 224696]  # SpO2 pulse oximetry\n",
    "fio2_itemids = [220210, 223835]  # FiO2 (%) and FiO2 (fraction)\n",
    "all_itemids = peep_itemids + spo2_itemids + fio2_itemids\n",
    "\n",
    "# Get all ICU stays for our cohort\n",
    "cohort_icustays = icustays[icustays['hadm_id'].isin(admissions_with_icu['hadm_id'])].copy()\n",
    "cohort_stay_ids = set(cohort_icustays['stay_id'])\n",
    "print(f\"ICU stays for adult cohort: {len(cohort_icustays):,}\")\n",
    "\n",
    "print(\"Loading chartevents data (this may take a few minutes)...\")\n",
    "# SINGLE pass through chartevents - load all needed data at once\n",
    "chartevents = pd.read_csv(\n",
    "    f'{MIMIC_PATH}/mimiciv/3.1/icu/chartevents.csv.gz',\n",
    "    usecols=['stay_id', 'itemid', 'charttime', 'valuenum']\n",
    ")\n",
    "\n",
    "print(f\"Total chartevents loaded: {len(chartevents):,}\")\n",
    "\n",
    "# Filter to our cohort and items in one operation (vectorized)\n",
    "cohort_data = chartevents[\n",
    "    (chartevents['stay_id'].isin(cohort_stay_ids)) &\n",
    "    (chartevents['itemid'].isin(all_itemids)) &\n",
    "    (chartevents['valuenum'].notna())\n",
    "].copy()\n",
    "\n",
    "print(f\"Relevant measurements for our cohort: {len(cohort_data):,}\")\n",
    "\n",
    "# Add parameter types (vectorized)\n",
    "cohort_data['param_type'] = 'unknown'\n",
    "cohort_data.loc[cohort_data['itemid'].isin(peep_itemids), 'param_type'] = 'peep'\n",
    "cohort_data.loc[cohort_data['itemid'].isin(spo2_itemids), 'param_type'] = 'spo2'\n",
    "cohort_data.loc[cohort_data['itemid'].isin(fio2_itemids), 'param_type'] = 'fio2'\n",
    "\n",
    "# Convert times and merge with ICU stay info\n",
    "cohort_data['charttime'] = pd.to_datetime(cohort_data['charttime'])\n",
    "cohort_data = cohort_data.merge(\n",
    "    cohort_icustays[['stay_id', 'hadm_id', 'intime']], \n",
    "    on='stay_id'\n",
    ")\n",
    "\n",
    "# Calculate hours from ICU admission (vectorized)\n",
    "cohort_data['hours_from_icu'] = (\n",
    "    cohort_data['charttime'] - cohort_data['intime']\n",
    ").dt.total_seconds() / 3600\n",
    "\n",
    "print(\"Processing PEEP criteria...\")\n",
    "# Process PEEP filter: PEEP ≥ 5 within first 48 hours (vectorized)\n",
    "peep_first_48h = cohort_data[\n",
    "    (cohort_data['param_type'] == 'peep') &\n",
    "    (cohort_data['hours_from_icu'] >= 0) &\n",
    "    (cohort_data['hours_from_icu'] <= 48) &\n",
    "    (cohort_data['valuenum'] >= 5)\n",
    "]\n",
    "\n",
    "admissions_with_peep = set(peep_first_48h['hadm_id'].unique())\n",
    "print(f\"Admissions with PEEP ≥5 in first 48h: {len(admissions_with_peep):,}\")\n",
    "\n",
    "# Filter data to PEEP-qualifying admissions for efficiency\n",
    "cohort_data_peep = cohort_data[cohort_data['hadm_id'].isin(admissions_with_peep)]\n",
    "print(f\"Measurements after PEEP filter: {len(cohort_data_peep):,}\")\n",
    "\n",
    "# Clear original chartevents to free memory\n",
    "del chartevents\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract SpO2 and FiO2 data (vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing S/F ratio criteria...\")\n",
    "\n",
    "spo2_data = cohort_data_peep[cohort_data_peep['param_type'] == 'spo2'].copy()\n",
    "fio2_data = cohort_data_peep[cohort_data_peep['param_type'] == 'fio2'].copy()\n",
    "\n",
    "print(f\"SpO2 measurements: {len(spo2_data):,}\")\n",
    "print(f\"FiO2 measurements: {len(fio2_data):,}\")\n",
    "\n",
    "if len(spo2_data) > 0 and len(fio2_data) > 0:\n",
    "    # Convert FiO2 percentages to fractions (vectorized)\n",
    "    fio2_data.loc[fio2_data['valuenum'] > 1, 'valuenum'] /= 100\n",
    "    \n",
    "    # Sort data for merge_asof (time-based matching)\n",
    "    spo2_data = spo2_data.sort_values(['hadm_id', 'charttime'])\n",
    "    fio2_data = fio2_data.sort_values(['hadm_id', 'charttime'])\n",
    "    \n",
    "    print(\"Matching SpO2 and FiO2 measurements within 2-hour windows...\")\n",
    "    # Use pandas merge_asof for efficient time-based matching\n",
    "    sf_ratios = pd.merge_asof(\n",
    "        spo2_data[['hadm_id', 'charttime', 'valuenum']],\n",
    "        fio2_data[['hadm_id', 'charttime', 'valuenum']],\n",
    "        on='charttime',\n",
    "        by='hadm_id',\n",
    "        tolerance=pd.Timedelta(hours=2),\n",
    "        direction='nearest',\n",
    "        suffixes=('_spo2', '_fio2')\n",
    "    )\n",
    "    \n",
    "    # Calculate S/F ratios (vectorized)\n",
    "    sf_ratios = sf_ratios.dropna(subset=['valuenum_fio2'])\n",
    "    sf_ratios = sf_ratios[sf_ratios['valuenum_fio2'] > 0]  # Avoid division by zero\n",
    "    sf_ratios['sf_ratio'] = sf_ratios['valuenum_spo2'] / sf_ratios['valuenum_fio2']\n",
    "    \n",
    "    print(f\"Successful S/F ratio calculations: {len(sf_ratios):,}\")\n",
    "    print(f\"S/F ratio distribution:\")\n",
    "    print(sf_ratios['sf_ratio'].describe())\n",
    "    \n",
    "    # Filter for S/F < 315\n",
    "    low_sf_ratios = sf_ratios[sf_ratios['sf_ratio'] < 315]\n",
    "    admissions_with_low_sf = set(low_sf_ratios['hadm_id'].unique())\n",
    "    \n",
    "    print(f\"\\nAdmissions with S/F ratio < 315: {len(admissions_with_low_sf):,}\")\n",
    "    \n",
    "    # Get final qualifying admissions (both PEEP and S/F criteria)\n",
    "    qualifying_admissions = admissions_with_peep.intersection(admissions_with_low_sf)\n",
    "    print(f\"Admissions meeting both PEEP and S/F criteria: {len(qualifying_admissions):,}\")\n",
    "    \n",
    "    # Filter cohort\n",
    "    cohort_with_criteria = admissions_with_icu[\n",
    "        admissions_with_icu['hadm_id'].isin(qualifying_admissions)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"Cohort after PEEP and S/F filters: {len(cohort_with_criteria):,}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Insufficient SpO2 or FiO2 data!\")\n",
    "    cohort_with_criteria = pd.DataFrame()\n",
    "\n",
    "# Clear processed data to free memory\n",
    "del cohort_data, cohort_data_peep\n",
    "if 'spo2_data' in locals():\n",
    "    del spo2_data, fio2_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for patients with radiology reports\n",
    "print(\"Filtering for patients with radiology reports...\")\n",
    "radiology = pd.read_csv(f'{MIMIC_PATH}/mimic-iv-note/2.2/note/radiology.csv.gz')\n",
    "print(f\"Total radiology reports: {len(radiology):,}\")\n",
    "\n",
    "# Get admissions with at least one radiology report\n",
    "admissions_with_radiology = set(radiology['hadm_id'].dropna().unique())\n",
    "print(f\"Admissions with radiology reports: {len(admissions_with_radiology):,}\")\n",
    "\n",
    "# Filter cohort to those with radiology reports\n",
    "if len(cohort_with_criteria) > 0:\n",
    "    cohort_with_radiology = cohort_with_criteria[\n",
    "        cohort_with_criteria['hadm_id'].isin(admissions_with_radiology)\n",
    "    ].copy()\n",
    "    print(f\"Cohort with radiology reports: {len(cohort_with_radiology):,}\")\n",
    "else:\n",
    "    cohort_with_radiology = pd.DataFrame()\n",
    "    print(\"No cohort data to filter for radiology!\")\n",
    "\n",
    "# Clear radiology data to free memory\n",
    "del radiology\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load radiology reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading radiology reports...\")\n",
    "radiology = pd.read_csv(f'{MIMIC_PATH}/mimic-iv-note/2.2/note/radiology.csv.gz')\n",
    "print(f\"Total radiology reports: {len(radiology):,}\")\n",
    "\n",
    "# Get patients with at least one radiology report\n",
    "patients_with_radiology = radiology[['subject_id', 'hadm_id']].drop_duplicates()\n",
    "print(f\"Unique patients with radiology: {patients_with_radiology['subject_id'].nunique():,}\")\n",
    "\n",
    "# Filter cohort to those with radiology reports\n",
    "cohort_with_radiology = cohort_with_sf.merge(\n",
    "    patients_with_radiology[['hadm_id']].drop_duplicates(), \n",
    "    on='hadm_id', \n",
    "    how='inner'\n",
    ")\n",
    "print(f\"\\nCohort with radiology reports: {len(cohort_with_radiology):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply exclusion criteria (Heart Failure and Pregnancy)\n",
    "print(\"Applying exclusion criteria...\")\n",
    "diagnoses = pd.read_csv(f'{MIMIC_PATH}/mimiciv/3.1/hosp/diagnoses_icd.csv.gz')\n",
    "\n",
    "# Heart failure ICD codes (vectorized)\n",
    "hf_icd9_codes = [str(x) for x in range(4280, 4290)]  # 428.0 - 428.9\n",
    "hf_icd10_codes = ['I50' + str(x) for x in range(10)] + ['I50']  # I50, I50.0 - I50.9\n",
    "\n",
    "hf_diagnoses = diagnoses[\n",
    "    (diagnoses['icd_code'].str.startswith(tuple(hf_icd9_codes))) |\n",
    "    (diagnoses['icd_code'].str.startswith(tuple(hf_icd10_codes)))\n",
    "]\n",
    "hf_hadm_ids = set(hf_diagnoses['hadm_id'].unique())\n",
    "print(f\"Admissions with heart failure: {len(hf_hadm_ids):,}\")\n",
    "\n",
    "# Pregnancy ICD codes (vectorized)\n",
    "pregnancy_icd9_prefixes = [str(x) for x in range(630, 680)]\n",
    "pregnancy_icd10_prefix = 'O'\n",
    "\n",
    "pregnancy_diagnoses = diagnoses[\n",
    "    (diagnoses['icd_code'].str[:3].isin(pregnancy_icd9_prefixes)) |\n",
    "    (diagnoses['icd_code'].str.startswith(pregnancy_icd10_prefix))\n",
    "]\n",
    "pregnant_hadm_ids = set(pregnancy_diagnoses['hadm_id'].unique())\n",
    "print(f\"Admissions with pregnancy codes: {len(pregnant_hadm_ids):,}\")\n",
    "\n",
    "# Apply exclusions if we have cohort data\n",
    "if len(cohort_with_radiology) > 0:\n",
    "    # Mark exclusion criteria\n",
    "    cohort_with_radiology['has_heart_failure'] = cohort_with_radiology['hadm_id'].isin(hf_hadm_ids)\n",
    "    cohort_with_radiology['is_pregnant'] = cohort_with_radiology['hadm_id'].isin(pregnant_hadm_ids)\n",
    "    \n",
    "    print(f\"\\nCohort patients with HF: {cohort_with_radiology['has_heart_failure'].sum():,}\")\n",
    "    print(f\"Cohort patients who are pregnant: {cohort_with_radiology['is_pregnant'].sum():,}\")\n",
    "    \n",
    "    # Apply exclusions (vectorized)\n",
    "    final_cohort = cohort_with_radiology[\n",
    "        (~cohort_with_radiology['has_heart_failure']) & \n",
    "        (~cohort_with_radiology['is_pregnant'])\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"\\nFinal cohort after exclusions: {len(final_cohort):,}\")\n",
    "    print(f\"Unique patients: {final_cohort['subject_id'].nunique():,}\")\n",
    "    \n",
    "    # Add admission and discharge times\n",
    "    final_cohort['admission_dttm'] = final_cohort['admittime']\n",
    "    final_cohort['discharge_dttm'] = final_cohort['dischtime']\n",
    "    \n",
    "else:\n",
    "    final_cohort = pd.DataFrame()\n",
    "    print(\"No cohort data to apply exclusions to!\")\n",
    "\n",
    "# Clear diagnoses data to free memory\n",
    "del diagnoses\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading diagnoses for heart failure identification...\n",
      "Admissions with heart failure: 80,611\n",
      "Cohort patients with HF: 17,000\n"
     ]
    }
   ],
   "source": [
    "# Load diagnoses\n",
    "print(\"Loading diagnoses for heart failure identification...\")\n",
    "diagnoses = pd.read_csv(f'{MIMIC_PATH}/mimiciv/3.1/hosp/diagnoses_icd.csv.gz')\n",
    "\n",
    "# Heart failure ICD codes\n",
    "# ICD-9: 428.x\n",
    "# ICD-10: I50.x\n",
    "hf_icd9_codes = [str(x) for x in range(4280, 4290)]  # 428.0 - 428.9\n",
    "hf_icd10_codes = ['I50' + str(x) for x in range(10)] + ['I50']  # I50, I50.0 - I50.9\n",
    "\n",
    "# Find patients with heart failure\n",
    "hf_diagnoses = diagnoses[\n",
    "    (diagnoses['icd_code'].str.startswith(tuple(hf_icd9_codes))) |\n",
    "    (diagnoses['icd_code'].str.startswith(tuple(hf_icd10_codes)))\n",
    "]\n",
    "\n",
    "hf_hadm_ids = set(hf_diagnoses['hadm_id'].unique())\n",
    "print(f\"Admissions with heart failure: {len(hf_hadm_ids):,}\")\n",
    "\n",
    "# Mark heart failure in cohort\n",
    "cohort_with_vent['has_heart_failure'] = cohort_with_vent['hadm_id'].isin(hf_hadm_ids)\n",
    "print(f\"Cohort patients with HF: {cohort_with_vent['has_heart_failure'].sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Identify Pregnant Patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading diagnoses for pregnant patients...\n",
      "Admissions with pregnancy codes: 26,549\n",
      "Cohort patients who are pregnant: 322\n"
     ]
    }
   ],
   "source": [
    "# Pregnancy ICD codes\n",
    "# ICD-9: 630-679 (pregnancy, childbirth, and the puerperium)\n",
    "# ICD-10: O00-O99 (pregnancy, childbirth and the puerperium)\n",
    "print(\"Loading diagnoses for pregnant patients...\")\n",
    "diagnoses = pd.read_csv(f'{MIMIC_PATH}/mimiciv/3.1/hosp/diagnoses_icd.csv.gz')\n",
    "\n",
    "# 398.91|402.01|402.11|402.91|404.01|404.03|404.11|404.13|404.91|404.93|428\n",
    "pregnancy_icd9_prefixes = [str(x) for x in range(630, 680)]\n",
    "pregnancy_icd10_prefix = 'O'\n",
    "\n",
    "# Find pregnant patients\n",
    "pregnancy_diagnoses = diagnoses[\n",
    "    (diagnoses['icd_code'].str[:3].isin(pregnancy_icd9_prefixes)) |\n",
    "    (diagnoses['icd_code'].str.startswith(pregnancy_icd10_prefix))\n",
    "]\n",
    "\n",
    "pregnant_hadm_ids = set(pregnancy_diagnoses['hadm_id'].unique())\n",
    "print(f\"Admissions with pregnancy codes: {len(pregnant_hadm_ids):,}\")\n",
    "\n",
    "# Mark pregnancy in cohort\n",
    "cohort_with_vent['is_pregnant'] = cohort_with_vent['hadm_id'].isin(pregnant_hadm_ids)\n",
    "print(f\"Cohort patients who are pregnant: {cohort_with_vent['is_pregnant'].sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohort patients who are pregnant and have HF: 19\n"
     ]
    }
   ],
   "source": [
    "# Find patients who are both pregnant and have heart failure\n",
    "pregnant_hf_hadm_ids = pregnant_hadm_ids.intersection(hf_hadm_ids)\n",
    "cohort_with_vent['is_pregnant_hf'] = cohort_with_vent['hadm_id'].isin(pregnant_hf_hadm_ids)\n",
    "print(f\"Cohort patients who are pregnant and have HF: {cohort_with_vent['is_pregnant_hf'].sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Apply Exclusion Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cohort after exclusions: 45,506\n",
      "Unique patients: 37,492\n"
     ]
    }
   ],
   "source": [
    "# Apply exclusions\n",
    "final_cohort = cohort_with_vent[\n",
    "    (~cohort_with_vent['has_heart_failure']) & \n",
    "    (~cohort_with_vent['is_pregnant'])\n",
    "].copy()\n",
    "\n",
    "print(f\"Final cohort after exclusions: {len(final_cohort):,}\")\n",
    "print(f\"Unique patients: {final_cohort['subject_id'].nunique():,}\")\n",
    "\n",
    "# Add admission and discharge times\n",
    "final_cohort['admission_dttm'] = final_cohort['admittime']\n",
    "final_cohort['discharge_dttm'] = final_cohort['dischtime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary statistics for the final cohort\n",
    "if len(final_cohort) > 0:\n",
    "    print(\"=== FINAL COHORT SUMMARY ===\")\n",
    "    print(f\"\\nTotal admissions: {len(final_cohort):,}\")\n",
    "    print(f\"Unique patients: {final_cohort['subject_id'].nunique():,}\")\n",
    "    \n",
    "    print(f\"\\nAge distribution:\")\n",
    "    print(final_cohort['age_at_admission'].describe())\n",
    "    \n",
    "    print(f\"\\nGender distribution:\")\n",
    "    print(final_cohort['gender'].value_counts())\n",
    "    \n",
    "    print(f\"\\nAdmission type:\")\n",
    "    print(final_cohort['admission_type'].value_counts())\n",
    "    \n",
    "    print(f\"\\nAdmission location:\")\n",
    "    print(final_cohort['admission_location'].value_counts().head(10))\n",
    "    \n",
    "    print(f\"\\nDischarge location:\")\n",
    "    print(final_cohort['discharge_location'].value_counts().head(10))\n",
    "    \n",
    "    # Hospital mortality\n",
    "    final_cohort['mortality'] = final_cohort['hospital_expire_flag']\n",
    "    print(f\"\\nHospital mortality: {final_cohort['mortality'].sum():,} ({final_cohort['mortality'].mean()*100:.1f}%)\")\n",
    "    \n",
    "    # ICU length of stay\n",
    "    icu_los = final_cohort['outtime'] - final_cohort['intime']\n",
    "    final_cohort['icu_los_days'] = icu_los.dt.total_seconds() / (24 * 3600)\n",
    "    print(f\"\\nICU Length of Stay (days):\")\n",
    "    print(final_cohort['icu_los_days'].describe())\n",
    "    \n",
    "else:\n",
    "    print(\"No final cohort data available for summary!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 8: Save Cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final cohort\n",
    "if len(final_cohort) > 0:\n",
    "    # Create output directory\n",
    "    import os\n",
    "    output_dir = '/Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/ards_analysis/data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Select key columns for cohort\n",
    "    cohort_columns = [\n",
    "        'subject_id', 'hadm_id', 'admission_dttm', 'discharge_dttm',\n",
    "        'age_at_admission', 'gender', 'admission_type',\n",
    "        'admission_location', 'discharge_location', 'insurance',\n",
    "        'marital_status', 'mortality', 'has_heart_failure', 'is_pregnant',\n",
    "        'icu_los_days', 'intime', 'outtime'\n",
    "    ]\n",
    "    \n",
    "    # Save cohort\n",
    "    cohort_file = f'{output_dir}/base_cohort_updated.parquet'\n",
    "    final_cohort[cohort_columns].to_parquet(cohort_file, index=False)\n",
    "    print(f\"\\nUpdated cohort saved to: {cohort_file}\")\n",
    "    print(f\"File size: {os.path.getsize(cohort_file) / 1024 / 1024:.1f} MB\")\n",
    "    \n",
    "    # Also save summary statistics\n",
    "    summary_stats = {\n",
    "        'total_admissions': len(final_cohort),\n",
    "        'unique_patients': final_cohort['subject_id'].nunique(),\n",
    "        'mean_age': final_cohort['age_at_admission'].mean(),\n",
    "        'mortality_rate': final_cohort['mortality'].mean(),\n",
    "        'mean_icu_los': final_cohort['icu_los_days'].mean()\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nCohort Summary:\")\n",
    "    for key, value in summary_stats.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "        \n",
    "else:\n",
    "    print(\"No final cohort to save!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cohort saved to: /Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/ards_analysis/data/base_cohort.parquet\n",
      "File size: 3.5 MB\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "import os\n",
    "output_dir = '/Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/ards_analysis/data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Select key columns for cohort\n",
    "cohort_columns = [\n",
    "    'subject_id', 'hadm_id', 'admission_dttm', 'discharge_dttm',\n",
    "    'age_at_admission', 'gender', 'admission_type',\n",
    "    'admission_location', 'discharge_location', 'insurance',\n",
    "    'marital_status', 'mortality', 'has_heart_failure', 'is_pregnant'\n",
    "]\n",
    "\n",
    "# Save cohort\n",
    "cohort_file = f'{output_dir}/base_cohort.parquet'\n",
    "final_cohort.to_parquet(cohort_file, index=False)\n",
    "print(f\"\\nCohort saved to: {cohort_file}\")\n",
    "print(f\"File size: {os.path.getsize(cohort_file) / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This cohort represents all adult patients with:\n",
    "- At least one ICU admission\n",
    "- PEEP ≥ 5 within first 48 hours of ICU admission\n",
    "- S/F ratio < 315 at least once (SpO2/FiO2)\n",
    "- At least one radiology report\n",
    "- No heart failure diagnosis\n",
    "- Not pregnant\n",
    "\n",
    "Next notebooks will:\n",
    "1. Apply Berlin criteria to identify ARDS patients\n",
    "2. Extract proning events\n",
    "3. Extract neuromuscular blockade administration\n",
    "4. Calculate timing metrics and outcomes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
