{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARDS Identification Using Berlin Criteria\n",
    "\n",
    "This notebook identifies ARDS patients from our base cohort using the Berlin Definition:\n",
    "\n",
    "## Berlin Criteria for ARDS:\n",
    "1. **Timing**: Within 1 week of known clinical insult or new/worsening respiratory symptoms\n",
    "2. **Chest imaging**: Bilateral opacities not fully explained by effusions, lobar/lung collapse, or nodules\n",
    "3. **Origin of edema**: Respiratory failure not fully explained by cardiac failure or fluid overload\n",
    "4. **Oxygenation** (with PEEP ≥ 5 cm H2O):\n",
    "   - Mild: 200 < PaO2/FiO2 ≤ 300\n",
    "   - Moderate: 100 < PaO2/FiO2 ≤ 200\n",
    "   - Severe: PaO2/FiO2 ≤ 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis start time: 2025-07-19 22:38:08.175920\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define paths\n",
    "MIMIC_PATH = '/Users/kavenchhikara/Desktop/CLIF/MIMIC-IV-3.1/physionet.org/files'\n",
    "DATA_PATH = '/Users/kavenchhikara/Desktop/projects/SCCM/SCCM-Team2/ards_analysis/data'\n",
    "\n",
    "print(f\"Analysis start time: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Base Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base cohort loaded: 45,506 admissions\n",
      "Unique patients: 37,492\n"
     ]
    }
   ],
   "source": [
    "# Load the base cohort from previous notebook\n",
    "base_cohort = pd.read_parquet(f'{DATA_PATH}/base_cohort.parquet')\n",
    "print(f\"Base cohort loaded: {len(base_cohort):,} admissions\")\n",
    "print(f\"Unique patients: {base_cohort['subject_id'].nunique():,}\")\n",
    "\n",
    "# Convert datetime columns\n",
    "base_cohort['admission_dttm'] = pd.to_datetime(base_cohort['admission_dttm'])\n",
    "base_cohort['discharge_dttm'] = pd.to_datetime(base_cohort['discharge_dttm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Radiology Reports for Bilateral Infiltrates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading radiology reports...\n",
      "Radiology reports for cohort: 358,141\n"
     ]
    }
   ],
   "source": [
    "# Load radiology reports\n",
    "print(\"Loading radiology reports...\")\n",
    "radiology = pd.read_csv(f'{MIMIC_PATH}/mimic-iv-note/2.2/note/radiology.csv.gz')\n",
    "\n",
    "# Filter to our cohort\n",
    "cohort_radiology = radiology[radiology['hadm_id'].isin(base_cohort['hadm_id'])].copy()\n",
    "print(f\"Radiology reports for cohort: {len(cohort_radiology):,}\")\n",
    "\n",
    "# Convert charttime\n",
    "cohort_radiology['charttime'] = pd.to_datetime(cohort_radiology['charttime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting bilateral infiltrates in radiology reports...\n",
      "Reports with bilateral infiltrates: 34,232 (9.6%)\n"
     ]
    }
   ],
   "source": [
    "# Function to detect bilateral infiltrates/opacities in radiology reports\n",
    "def detect_bilateral_infiltrates(text):\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Bilateral patterns\n",
    "    bilateral_patterns = [\n",
    "        r'bilateral.*(?:infiltrate|opacity|opacities|consolidation)',\n",
    "        r'(?:infiltrate|opacity|opacities|consolidation).*bilateral',\n",
    "        r'both lung.*(?:infiltrate|opacity|opacities|consolidation)',\n",
    "        r'(?:infiltrate|opacity|opacities|consolidation).*both lung',\n",
    "        r'diffuse.*(?:infiltrate|opacity|opacities|consolidation)',\n",
    "        r'multifocal.*(?:infiltrate|opacity|opacities|consolidation)',\n",
    "        r'bibasilar.*(?:infiltrate|opacity|opacities|consolidation)',\n",
    "        r'bilateral.*ground.?glass',\n",
    "        r'bilateral.*airspace disease',\n",
    "        r'ards', # Direct ARDS mention\n",
    "        r'acute respiratory distress syndrome'\n",
    "    ]\n",
    "    \n",
    "    # Check for any bilateral pattern\n",
    "    for pattern in bilateral_patterns:\n",
    "        if re.search(pattern, text_lower):\n",
    "            return True\n",
    "    \n",
    "    # Check for infiltrates in both lungs mentioned separately\n",
    "    left_infiltrate = re.search(r'left.*(?:infiltrate|opacity|consolidation)', text_lower)\n",
    "    right_infiltrate = re.search(r'right.*(?:infiltrate|opacity|consolidation)', text_lower)\n",
    "    \n",
    "    return bool(left_infiltrate and right_infiltrate)\n",
    "\n",
    "# Apply detection\n",
    "print(\"Detecting bilateral infiltrates in radiology reports...\")\n",
    "cohort_radiology['bilateral_infiltrates'] = cohort_radiology['text'].apply(detect_bilateral_infiltrates)\n",
    "print(f\"Reports with bilateral infiltrates: {cohort_radiology['bilateral_infiltrates'].sum():,} ({cohort_radiology['bilateral_infiltrates'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Admissions with bilateral infiltrates: 14,557\n"
     ]
    }
   ],
   "source": [
    "# Get first occurrence of bilateral infiltrates for each admission\n",
    "bilateral_infiltrates_df = cohort_radiology[cohort_radiology['bilateral_infiltrates']].groupby('hadm_id').agg({\n",
    "    'charttime': 'min',  # First occurrence\n",
    "    'note_id': 'first'\n",
    "}).reset_index()\n",
    "bilateral_infiltrates_df.rename(columns={'charttime': 'bilateral_infiltrates_time'}, inplace=True)\n",
    "\n",
    "print(f\"Admissions with bilateral infiltrates: {len(bilateral_infiltrates_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract Ventilation Parameters (PEEP, PaO2, FiO2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICU stays for cohort: 49,755\n"
     ]
    }
   ],
   "source": [
    "# Load ICU stays to link hadm_id with stay_id\n",
    "icustays = pd.read_csv(f'{MIMIC_PATH}/mimiciv/3.1/icu/icustays.csv.gz')\n",
    "cohort_icustays = icustays[icustays['hadm_id'].isin(base_cohort['hadm_id'])].copy()\n",
    "print(f\"ICU stays for cohort: {len(cohort_icustays):,}\")\n",
    "\n",
    "# Define itemids for ventilation parameters\n",
    "ventilation_itemids = {\n",
    "    'peep': [220339, 224700],  # PEEP\n",
    "    'fio2_percent': [220210],  # FiO2 (%)\n",
    "    'fio2_fraction': [223835, 224698],  # FiO2 (fraction)\n",
    "    'pao2': [220224, 224689],  # PaO2\n",
    "    'pf_ratio': [223834]  # PaO2/FiO2 ratio (if directly recorded)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ventilation parameters...\n",
      "Processed 1,000,000 rows...\n",
      "Processed 11,000,000 rows...\n",
      "Processed 21,000,000 rows...\n",
      "Processed 31,000,000 rows...\n",
      "Processed 41,000,000 rows...\n",
      "Processed 51,000,000 rows...\n",
      "Processed 61,000,000 rows...\n",
      "Processed 71,000,000 rows...\n",
      "Processed 81,000,000 rows...\n",
      "Processed 91,000,000 rows...\n",
      "Processed 101,000,000 rows...\n",
      "Processed 111,000,000 rows...\n",
      "Processed 121,000,000 rows...\n",
      "Processed 131,000,000 rows...\n",
      "Processed 141,000,000 rows...\n",
      "Processed 151,000,000 rows...\n",
      "Processed 161,000,000 rows...\n",
      "Processed 171,000,000 rows...\n",
      "Processed 181,000,000 rows...\n",
      "Processed 191,000,000 rows...\n",
      "Processed 201,000,000 rows...\n",
      "Processed 211,000,000 rows...\n",
      "Processed 221,000,000 rows...\n",
      "Processed 231,000,000 rows...\n",
      "Processed 241,000,000 rows...\n",
      "Processed 251,000,000 rows...\n",
      "Processed 261,000,000 rows...\n",
      "Processed 271,000,000 rows...\n",
      "Processed 281,000,000 rows...\n",
      "Processed 291,000,000 rows...\n",
      "Processed 301,000,000 rows...\n",
      "Processed 311,000,000 rows...\n",
      "Processed 321,000,000 rows...\n",
      "Processed 331,000,000 rows...\n",
      "Processed 341,000,000 rows...\n",
      "Processed 351,000,000 rows...\n",
      "Processed 361,000,000 rows...\n",
      "Processed 371,000,000 rows...\n",
      "Processed 381,000,000 rows...\n",
      "Processed 391,000,000 rows...\n",
      "Processed 401,000,000 rows...\n",
      "Processed 411,000,000 rows...\n",
      "Processed 421,000,000 rows...\n",
      "Processed 431,000,000 rows...\n",
      "\n",
      "Total ventilation measurements: 6,384,805\n"
     ]
    }
   ],
   "source": [
    "# Extract ventilation parameters\n",
    "print(\"Extracting ventilation parameters...\")\n",
    "all_itemids = [item for items in ventilation_itemids.values() for item in items]\n",
    "\n",
    "# Load chartevents for our cohort's ICU stays\n",
    "vent_data = []\n",
    "chunk_size = 1000000\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(f'{MIMIC_PATH}/mimiciv/3.1/icu/chartevents.csv.gz', \n",
    "                                     chunksize=chunk_size)):\n",
    "    # Filter for our cohort's stays and relevant items\n",
    "    chunk_filtered = chunk[\n",
    "        (chunk['stay_id'].isin(cohort_icustays['stay_id'])) &\n",
    "        (chunk['itemid'].isin(all_itemids))\n",
    "    ]\n",
    "    \n",
    "    if len(chunk_filtered) > 0:\n",
    "        vent_data.append(chunk_filtered)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processed {(i+1)*chunk_size:,} rows...\")\n",
    "\n",
    "# Combine all ventilation data\n",
    "vent_df = pd.concat(vent_data, ignore_index=True)\n",
    "print(f\"\\nTotal ventilation measurements: {len(vent_df):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject_id', 'hadm_id', 'stay_id', 'first_careunit', 'last_careunit',\n",
       "       'intime', 'outtime', 'los'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort_icustays.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject_id', 'hadm_id', 'stay_id', 'caregiver_id', 'charttime',\n",
       "       'storetime', 'itemid', 'value', 'valuenum', 'valueuom', 'warning'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vent_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add hadm_id from icustays\n",
    "# vent_df = vent_df.merge(cohort_icustays[['stay_id', 'hadm_id']], on='stay_id', how='left')\n",
    "\n",
    "# Convert charttime\n",
    "vent_df['charttime'] = pd.to_datetime(vent_df['charttime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ventilation parameters (vectorized)...\n",
      "Processed ventilation measurements: 4,637,866\n",
      "Measurements with PEEP: 450,594\n",
      "Measurements with P/F ratio: 748,916\n"
     ]
    }
   ],
   "source": [
    "def process_vent_params_vectorized(df):\n",
    "    \"\"\"\n",
    "    Vectorized version - much faster than the loop-based approach\n",
    "    \"\"\"\n",
    "    print(\"Processing ventilation parameters (vectorized)...\")\n",
    "\n",
    "    # Create a copy to avoid modifying original\n",
    "    df = df.copy()\n",
    "\n",
    "    # Add parameter type column\n",
    "    df['param_type'] = 'unknown'\n",
    "    df.loc[df['itemid'].isin(ventilation_itemids['peep']), 'param_type'] = 'peep'\n",
    "    df.loc[df['itemid'].isin(ventilation_itemids['fio2_percent']), 'param_type'] = 'fio2_percent'\n",
    "    df.loc[df['itemid'].isin(ventilation_itemids['fio2_fraction']), 'param_type'] = 'fio2_fraction'\n",
    "    df.loc[df['itemid'].isin(ventilation_itemids['pao2']), 'param_type'] = 'pao2'\n",
    "    df.loc[df['itemid'].isin(ventilation_itemids['pf_ratio']), 'param_type'] = 'pf_ratio'\n",
    "\n",
    "    # Convert FiO2 percentage to fraction\n",
    "    df.loc[df['param_type'] == 'fio2_percent', 'valuenum'] = df.loc[df['param_type'] == 'fio2_percent', 'valuenum'] / 100\n",
    "\n",
    "    # Combine FiO2 types\n",
    "    df.loc[df['param_type'] == 'fio2_percent', 'param_type'] = 'fio2'\n",
    "    df.loc[df['param_type'] == 'fio2_fraction', 'param_type'] = 'fio2'\n",
    "\n",
    "    # Pivot to get one row per hadm_id/charttime with columns for each parameter\n",
    "    pivot_df = df.pivot_table(\n",
    "        index=['hadm_id', 'charttime'],\n",
    "        columns='param_type',\n",
    "        values='valuenum',\n",
    "        aggfunc='first'  # Take first value if multiple\n",
    "    ).reset_index()\n",
    "\n",
    "    # Flatten column names\n",
    "    pivot_df.columns.name = None\n",
    "\n",
    "    # Ensure all expected columns exist\n",
    "    for col in ['peep', 'fio2', 'pao2', 'pf_ratio']:\n",
    "        if col not in pivot_df.columns:\n",
    "            pivot_df[col] = None\n",
    "\n",
    "    # Calculate P/F ratio where not directly available\n",
    "    mask = (pivot_df['pf_ratio'].isna()) & (pivot_df['pao2'].notna()) &(pivot_df['fio2'].notna()) & (pivot_df['fio2'] > 0)\n",
    "    pivot_df.loc[mask, 'pf_ratio'] = pivot_df.loc[mask, 'pao2'] / pivot_df.loc[mask, 'fio2']\n",
    "\n",
    "    return pivot_df[['hadm_id', 'charttime', 'peep', 'fio2', 'pao2', 'pf_ratio']]\n",
    "\n",
    "# Use the vectorized version\n",
    "vent_processed = process_vent_params_vectorized(vent_df)\n",
    "\n",
    "print(f\"Processed ventilation measurements: {len(vent_processed):,}\")\n",
    "print(f\"Measurements with PEEP: {vent_processed['peep'].notna().sum():,}\")\n",
    "print(f\"Measurements with P/F ratio: {vent_processed['pf_ratio'].notna().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Apply Berlin Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Identify ARDS cases based on Berlin criteria - CORRECTED VERSION\n# Using admission time as clinical insult proxy\ndef identify_ards_onset_corrected(hadm_id, bilateral_infiltrates_df, vent_processed, base_cohort):\n    \"\"\"\n    Identify ARDS onset for a given admission using CORRECT Berlin criteria timing\n    Returns: dict with ARDS onset info or None\n    \n    Correct Berlin timing: \n    - Clinical insult = admission time\n    - Bilateral infiltrates must be documented within 7 days of admission\n    - ARDS onset (meeting oxygenation criteria) must be within 7 days of admission\n    - ARDS onset must be AT OR AFTER bilateral infiltrates documentation\n    \"\"\"\n    # Check if patient has bilateral infiltrates\n    if hadm_id not in bilateral_infiltrates_df['hadm_id'].values:\n        return None\n    \n    bilateral_time = bilateral_infiltrates_df[bilateral_infiltrates_df['hadm_id'] == hadm_id]['bilateral_infiltrates_time'].iloc[0]\n    admission_time = base_cohort[base_cohort['hadm_id'] == hadm_id]['admission_dttm'].iloc[0]\n    \n    # Check if bilateral infiltrates are within 7 days of admission (clinical insult)\n    if (bilateral_time - admission_time).total_seconds() / (24 * 3600) > 7:\n        return None\n    \n    # Get ventilation data for this admission\n    hadm_vent = vent_processed[vent_processed['hadm_id'] == hadm_id].copy()\n    \n    if len(hadm_vent) == 0:\n        return None\n    \n    # ARDS criteria must be met within 7 days of clinical insult (admission)\n    # AND at or after bilateral infiltrates documentation\n    time_window_end = admission_time + timedelta(days=7)\n    \n    qualifying_measurements = hadm_vent[\n        (hadm_vent['charttime'] >= bilateral_time) &      # Must be after/at infiltrates documented\n        (hadm_vent['charttime'] >= admission_time) &      # Must be after admission\n        (hadm_vent['charttime'] <= time_window_end) &     # Within 7 days of admission (clinical insult)\n        (hadm_vent['peep'] >= 5) &                        # PEEP ≥ 5 cm H2O\n        (hadm_vent['pf_ratio'].notna()) &\n        (hadm_vent['pf_ratio'] <= 300)                    # P/F ratio ≤ 300\n    ].copy()\n    \n    if len(qualifying_measurements) == 0:\n        return None\n    \n    # Find the earliest qualifying measurement\n    qualifying_measurements = qualifying_measurements.sort_values('charttime')\n    first_qualifying = qualifying_measurements.iloc[0]\n    \n    # Determine ARDS severity\n    pf_ratio = first_qualifying['pf_ratio']\n    if pf_ratio <= 100:\n        severity = 'severe'\n    elif pf_ratio <= 200:\n        severity = 'moderate'\n    else:\n        severity = 'mild'\n    \n    return {\n        'hadm_id': hadm_id,\n        'ards_onset_time': first_qualifying['charttime'],\n        'bilateral_infiltrates_time': bilateral_time,\n        'admission_time': admission_time,\n        'initial_pf_ratio': pf_ratio,\n        'initial_peep': first_qualifying['peep'],\n        'initial_fio2': first_qualifying['fio2'],\n        'initial_pao2': first_qualifying['pao2'],\n        'ards_severity': severity\n    }\n\n# Vectorized version with admission time as clinical insult\ndef identify_ards_onset_vectorized_corrected(bilateral_infiltrates_df, vent_processed, base_cohort):\n    \"\"\"\n    Vectorized ARDS identification with admission time as clinical insult proxy\n    \"\"\"\n    print(\"Identifying ARDS cases (vectorized, admission time as clinical insult)...\")\n    \n    # Merge ventilation data with bilateral infiltrates and admission info\n    vent_with_bilateral = vent_processed.merge(\n        bilateral_infiltrates_df[['hadm_id', 'bilateral_infiltrates_time']], \n        on='hadm_id', \n        how='inner'\n    ).merge(\n        base_cohort[['hadm_id', 'admission_dttm']], \n        on='hadm_id', \n        how='inner'\n    )\n    \n    if len(vent_with_bilateral) == 0:\n        return pd.DataFrame()\n    \n    # Calculate time differences (vectorized)\n    vent_with_bilateral['hours_from_admission'] = (\n        vent_with_bilateral['charttime'] - vent_with_bilateral['admission_dttm']\n    ).dt.total_seconds() / 3600  # Hours from admission\n    \n    vent_with_bilateral['hours_from_bilateral'] = (\n        vent_with_bilateral['charttime'] - vent_with_bilateral['bilateral_infiltrates_time']\n    ).dt.total_seconds() / 3600  # Hours from bilateral infiltrates\n    \n    vent_with_bilateral['bilateral_from_admission'] = (\n        vent_with_bilateral['bilateral_infiltrates_time'] - vent_with_bilateral['admission_dttm']\n    ).dt.total_seconds() / 3600  # Hours from admission to bilateral infiltrates\n    \n    # Apply CORRECTED Berlin criteria filters\n    qualifying_measurements = vent_with_bilateral[\n        (vent_with_bilateral['bilateral_from_admission'] <= 168) &    # Bilateral infiltrates within 7 days of admission\n        (vent_with_bilateral['hours_from_bilateral'] >= 0) &         # Must be AT OR AFTER infiltrates\n        (vent_with_bilateral['hours_from_admission'] >= 0) &         # Must be after admission\n        (vent_with_bilateral['hours_from_admission'] <= 168) &       # Within 7 days of admission (clinical insult)\n        (vent_with_bilateral['peep'] >= 5) &                         # PEEP ≥ 5 cm H2O\n        (vent_with_bilateral['pf_ratio'].notna()) &\n        (vent_with_bilateral['pf_ratio'] <= 300)                     # P/F ratio ≤ 300\n    ].copy()\n    \n    if len(qualifying_measurements) == 0:\n        return pd.DataFrame()\n    \n    # Find earliest qualifying measurement for each patient (vectorized)\n    earliest_ards = qualifying_measurements.loc[\n        qualifying_measurements.groupby('hadm_id')['charttime'].idxmin()\n    ].copy()\n    \n    # Determine ARDS severity (vectorized)\n    severity_conditions = [\n        earliest_ards['pf_ratio'] <= 100,\n        earliest_ards['pf_ratio'] <= 200,\n        earliest_ards['pf_ratio'] <= 300\n    ]\n    severity_choices = ['severe', 'moderate', 'mild']\n    earliest_ards['ards_severity'] = np.select(severity_conditions, severity_choices, default='unknown')\n    \n    # Rename and select columns\n    ards_df = earliest_ards[[\n        'hadm_id', 'charttime', 'bilateral_infiltrates_time', 'admission_dttm',\n        'pf_ratio', 'peep', 'fio2', 'pao2', 'ards_severity', \n        'hours_from_admission', 'hours_from_bilateral', 'bilateral_from_admission'\n    ]].rename(columns={\n        'charttime': 'ards_onset_time',\n        'admission_dttm': 'admission_time',\n        'pf_ratio': 'initial_pf_ratio',\n        'peep': 'initial_peep',\n        'fio2': 'initial_fio2',\n        'pao2': 'initial_pao2'\n    })\n    \n    return ards_df\n\n# Use the vectorized corrected version with admission time as clinical insult\nards_df = identify_ards_onset_vectorized_corrected(bilateral_infiltrates_df, vent_processed, base_cohort)\n\nprint(f\"ARDS cases identified (admission time as clinical insult): {len(ards_df):,}\")\nif len(ards_df) > 0:\n    print(f\"\\nARDS severity distribution:\")\n    print(ards_df['ards_severity'].value_counts())\n    \n    print(f\"\\nTiming analysis:\")\n    print(f\"Hours from admission to bilateral infiltrates:\")\n    print(ards_df['bilateral_from_admission'].describe())\n    \n    print(f\"\\nHours from admission to ARDS onset:\")\n    print(ards_df['hours_from_admission'].describe())\n    \n    print(f\"\\nHours from bilateral infiltrates to ARDS onset:\")\n    print(ards_df['hours_from_bilateral'].describe())\n    \n    print(f\"\\nARDS onset timing distribution:\")\n    print(f\"Within 24 hours of admission: {(ards_df['hours_from_admission'] <= 24).sum()} ({(ards_df['hours_from_admission'] <= 24).mean()*100:.1f}%)\")\n    print(f\"Within 72 hours of admission: {(ards_df['hours_from_admission'] <= 72).sum()} ({(ards_df['hours_from_admission'] <= 72).mean()*100:.1f}%)\")\n    print(f\"Within 7 days of admission: {(ards_df['hours_from_admission'] <= 168).sum()} ({(ards_df['hours_from_admission'] <= 168).mean()*100:.1f}%)\")\n    \n    print(f\"\\nSame day as bilateral infiltrates: {(ards_df['hours_from_bilateral'] < 24).sum()}\")\n    print(f\"ARDS onset exactly at bilateral infiltrates time: {(ards_df['hours_from_bilateral'] == 0).sum()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Final ARDS Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge ARDS information with base cohort\n",
    "ards_cohort = base_cohort.merge(ards_df, on='hadm_id', how='inner')\n",
    "\n",
    "# Calculate time from admission to ARDS onset\n",
    "ards_cohort['ards_onset_time'] = pd.to_datetime(ards_cohort['ards_onset_time'])\n",
    "ards_cohort['hours_to_ards_onset'] = (ards_cohort['ards_onset_time'] - ards_cohort['admission_dttm']).dt.total_seconds() / 3600\n",
    "\n",
    "print(f\"Final ARDS cohort: {len(ards_cohort):,} patients\")\n",
    "print(f\"\\nTime to ARDS onset (hours):\")\n",
    "print(ards_cohort['hours_to_ards_onset'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ARDS COHORT SUMMARY ===\")\n",
    "print(f\"\\nTotal ARDS patients: {len(ards_cohort):,}\")\n",
    "print(f\"Unique subjects: {ards_cohort['subject_id'].nunique():,}\")\n",
    "\n",
    "print(f\"\\nAge distribution:\")\n",
    "print(ards_cohort['age_at_admission'].describe())\n",
    "\n",
    "print(f\"\\nGender distribution:\")\n",
    "print(ards_cohort['gender'].value_counts())\n",
    "\n",
    "print(f\"\\nARDS severity:\")\n",
    "print(ards_cohort['ards_severity'].value_counts())\n",
    "print(f\"\\nSeverity percentages:\")\n",
    "print(ards_cohort['ards_severity'].value_counts(normalize=True) * 100)\n",
    "\n",
    "print(f\"\\nInitial P/F ratio by severity:\")\n",
    "print(ards_cohort.groupby('ards_severity')['initial_pf_ratio'].agg(['mean', 'std', 'min', 'max']))\n",
    "\n",
    "print(f\"\\nHospital mortality:\")\n",
    "print(f\"Overall: {ards_cohort['hospital_expire_flag'].mean()*100:.1f}%\")\n",
    "print(\"\\nBy severity:\")\n",
    "print(ards_cohort.groupby('ards_severity')['hospital_expire_flag'].agg(['sum', 'mean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save ARDS Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ARDS cohort\n",
    "ards_cohort_file = f'{DATA_PATH}/ards_cohort.parquet'\n",
    "ards_cohort.to_parquet(ards_cohort_file, index=False)\n",
    "print(f\"\\nARDS cohort saved to: {ards_cohort_file}\")\n",
    "\n",
    "# Also save the ventilation data for later use\n",
    "vent_file = f'{DATA_PATH}/ventilation_parameters.parquet'\n",
    "vent_processed.to_parquet(vent_file, index=False)\n",
    "print(f\"Ventilation parameters saved to: {vent_file}\")\n",
    "\n",
    "print(f\"\\nAnalysis completed at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "We have successfully identified ARDS patients using Berlin criteria:\n",
    "- Bilateral infiltrates detected from radiology reports\n",
    "- PEEP ≥ 5 cm H2O\n",
    "- P/F ratio ≤ 300\n",
    "- Within 7-day time window\n",
    "\n",
    "Next notebooks will:\n",
    "1. Extract proning events from nursing documentation\n",
    "2. Extract neuromuscular blockade administration\n",
    "3. Calculate timing from ARDS onset\n",
    "4. Analyze outcomes (mortality, LOS, extubation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}